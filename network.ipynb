{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed\\AppData\\Local\\Temp\\ipykernel_10476\\3883581343.py:193: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"model.pth\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x102 and 64x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 197\u001b[0m\n\u001b[0;32m    193\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m agent \u001b[38;5;241m=\u001b[39m QLearningAgent(n, network\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m--> 197\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# print board\u001b[39;00m\n\u001b[0;32m    199\u001b[0m agent\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mprint_board()\n",
      "Cell \u001b[1;32mIn[24], line 162\u001b[0m, in \u001b[0;36mQLearningAgent.train\u001b[1;34m(self, iterations, max_steps)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    161\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mencode_state()\n\u001b[1;32m--> 162\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_q_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m tmp_epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n",
      "Cell \u001b[1;32mIn[24], line 119\u001b[0m, in \u001b[0;36mQLearningAgent.get_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return Q-values for a given state.\"\"\"\u001b[39;00m\n\u001b[0;32m    118\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32 , requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 119\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_values\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[24], line 16\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x102 and 64x128)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Neural Network definition\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "class Board:\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.queen_cols = [random.randint(0, n - 1) for _ in range(n)]\n",
    "        self.cols, self.plus_diags, self.minus_diags, self.conflicts = self.count_conflicts()\n",
    "\n",
    "    def count_conflicts(self):\n",
    "        n = self.n\n",
    "        cols = [0] * n\n",
    "        plus_diags = [0] * (2 * n - 1)\n",
    "        minus_diags = [0] * (2 * n - 1)\n",
    "        conflicts = 0\n",
    "\n",
    "        for row in range(n):\n",
    "            col = self.queen_cols[row]\n",
    "            conflicts += cols[col] + plus_diags[row + col] + minus_diags[row - col + n - 1]\n",
    "            cols[col] += 1\n",
    "            plus_diags[row + col] += 1\n",
    "            minus_diags[row - col + n - 1] += 1\n",
    "\n",
    "        return cols, plus_diags, minus_diags, conflicts\n",
    "    \n",
    "    def move_queen(self, row, new_col):\n",
    "        n = self.n\n",
    "        delta_conflicts = 0\n",
    "        \n",
    "        old_col = self.queen_cols[row]\n",
    "        self.queen_cols[row] = new_col\n",
    "\n",
    "        self.cols[old_col] -= 1\n",
    "        self.plus_diags[row + old_col] -= 1\n",
    "        self.minus_diags[row - old_col + n - 1] -= 1\n",
    "        delta_conflicts -= self.cols[old_col] + self.plus_diags[row + old_col] + self.minus_diags[row - old_col + n - 1]\n",
    "        \n",
    "        delta_conflicts += self.cols[new_col] + self.plus_diags[row + new_col] + self.minus_diags[row - new_col + n - 1]\n",
    "        self.cols[new_col] += 1\n",
    "        self.plus_diags[row + new_col] += 1\n",
    "        self.minus_diags[row - new_col + n - 1] += 1\n",
    "        \n",
    "        self.conflicts += delta_conflicts\n",
    "        return delta_conflicts\n",
    "\n",
    "\n",
    "    def encode_state(self):\n",
    "        \"\"\"Encode board state with proper dimensions\"\"\"\n",
    "        n = self.n\n",
    "        board_state = np.zeros(n * n)\n",
    "        for row, col in enumerate(self.queen_cols):\n",
    "            board_state[row * n + col] = 1\n",
    "        return np.concatenate([\n",
    "            board_state,\n",
    "            self.cols,\n",
    "            self.plus_diags,\n",
    "            self.minus_diags\n",
    "        ])\n",
    "\n",
    "    def decode_action(self, action):\n",
    "        \"\"\"Decode an action index into a (row, col) tuple.\"\"\"\n",
    "        # action is place on tile, for example, action =17 means its on the 17th tile(3rd row, second column)\n",
    "        n = self.n\n",
    "        row = action // n\n",
    "        col = action % n\n",
    "        return row, col\n",
    "    \n",
    "    def print_board(self):\n",
    "        \"\"\"Print the board.\"\"\"\n",
    "        n = self.n\n",
    "        for row in range(n):\n",
    "            for col in range(n):\n",
    "                if col == self.queen_cols[row]:\n",
    "                    print(\"Q\", end=\" \")\n",
    "                else:\n",
    "                    print(\".\", end=\" \")\n",
    "            print()\n",
    "        print()\n",
    "    \n",
    "\n",
    "# Simulated Annealing with Q-Network\n",
    "class QLearningAgent:\n",
    "    \n",
    "    def __init__(self, n, epsilon=0.04, decay_rate=0.99, network=None):\n",
    "        self.n = n\n",
    "        self.board = Board(n)\n",
    "        if network is None:\n",
    "            self.network = QNetwork(n_inputs=n*n*len(self.board.encode_state()), n_outputs=n * n)\n",
    "        else:\n",
    "            self.network = network\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=0.01)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.epsilon = epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def get_q_values(self, state):\n",
    "        \"\"\"Return Q-values for a given state.\"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32 , requires_grad=True).unsqueeze(0)\n",
    "        q_values = self.network(state_tensor)\n",
    "        return q_values.squeeze(0).detach().numpy()\n",
    "\n",
    "    def choose_action(self, q_values):\n",
    "        \"\"\"Choose an action using epsilon-greedy exploration.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n * self.n - 1)\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    # =========================================================================================\n",
    "    def update_network(self, q_values, action, reward, next_q_values):\n",
    "       \"\"\"Train the Q-network with a single step.\"\"\"\n",
    "       # Convert state and next_state to tensors\n",
    "    #    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    #    next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "   \n",
    "       # Convert q_values and next_q_values to tensors\n",
    "       q_values_tensor = torch.tensor(q_values, dtype=torch.float32, requires_grad=True)\n",
    "       next_q_values_tensor = torch.tensor(next_q_values, dtype=torch.float32, requires_grad=True)\n",
    "   \n",
    "         # Update the Q-value of the chosen action\n",
    "       target_q_values = q_values_tensor.clone().detach()\n",
    "       target_q_values[action] = reward + 0.9 * torch.max(next_q_values_tensor)\n",
    "   \n",
    "       # Calculate loss\n",
    "       loss = self.criterion(q_values_tensor, target_q_values)\n",
    "   \n",
    "       # Backpropagation and optimization step\n",
    "       self.optimizer.zero_grad()\n",
    "       loss.backward()\n",
    "       self.optimizer.step()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ========================================\n",
    "    def train(self, iterations=1000, max_steps=500):\n",
    "        \"\"\"Train the Q-network using epsilon-greedy policy.\"\"\"\n",
    "        \n",
    "        success = 0\n",
    "        for i in range(iterations):\n",
    "            self.board.reset()\n",
    "            state = self.board.encode_state()\n",
    "            q_values = self.get_q_values(state)\n",
    "            tmp_epsilon = self.epsilon\n",
    "            for step in range(max_steps):\n",
    "                occupied_positions = np.where(self.board.encode_state() == 1)[0]\n",
    "                q_values_tmp = q_values.copy()\n",
    "                q_values_tmp[occupied_positions] = -np.inf\n",
    "                action = self.choose_action(q_values_tmp)\n",
    "                prev_state = state.copy()\n",
    "                row, col = self.board.decode_action(action)\n",
    "                \n",
    "                delta_conflicts = self.board.move_queen(row, col)\n",
    "                state = self.board.encode_state()\n",
    "\n",
    "                reward = -delta_conflicts - 1 if self.board.conflicts > 0 else 100 \n",
    "            \n",
    "                next_q_values = self.get_q_values(state)\n",
    "                \n",
    "                self.update_network(q_values, action, reward, next_q_values)\n",
    "                q_values = next_q_values\n",
    "\n",
    "                if self.board.conflicts == 0:\n",
    "                    success += 1\n",
    "                    break\n",
    "\n",
    "                tmp_epsilon *= self.decay_rate\n",
    "                \n",
    "            print(f\"Iteration {i + 1}: {step + 1} steps, conflicts: {self.board.conflicts}, successes: {success}\")\n",
    "\n",
    "    \n",
    "n = 8\n",
    "# Training the agent\n",
    "model = torch.load(\"model.pth\")\n",
    "\n",
    "agent = QLearningAgent(n, network=model)\n",
    "\n",
    "agent.train()\n",
    "# print board\n",
    "agent.board.print_board()\n",
    "\n",
    "\n",
    "save_path = \"model.pth\"\n",
    "torch.save(agent.network, save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
