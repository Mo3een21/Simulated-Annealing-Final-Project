
---

# ğŸ§  Advanced 15-Puzzle Solver with Reinforcement Learning

This project implements a powerful and adaptive solver for the classic **15-Puzzle** using a combination of:

* **Reinforcement Learning** (with weighted heuristic learning),
* **Simulated Annealing** search,
* **Curriculum Learning** to adapt difficulty over time.

The goal is to **learn better heuristics** (than classic Manhattan distance) to solve more difficult puzzle configurations efficiently.

---

## ğŸš€ Features

* âœ… Simulated annealing-based solving
* âœ… Reinforcement learning of weighted heuristics
* âœ… Curriculum manager to adapt difficulty dynamically
* âœ… Linear conflict enhanced baseline heuristic
* âœ… Full tracking of training statistics (success rates, steps, losses)
* âœ… Evaluation framework comparing learned vs standard heuristics
* âœ… Visual training plots (via `matplotlib`)

---

## ğŸ“¦ Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/Mo3een21/15puzzle.git
   cd rl-15puzzle-solver
   ```

2. Install required Python packages:

   ```bash
   pip install -r requirements.txt
   ```

   Or install manually:

   ```bash
   pip install numpy matplotlib
   ```

---

## ğŸ§ª Running the Solver

```bash
python main.py
```

This will:

* Train the solver over multiple episodes using curriculum learning
* Learn heuristic weights using gradient descent
* Evaluate and compare performance against traditional heuristic
* Display final learned weights
* Plot progress statistics

---

## ğŸ§  Architecture Overview

```
.
â”œâ”€â”€ Puzzle15Env            # Core 15-puzzle environment with linear conflict + learned heuristic support
â”œâ”€â”€ ImprovedWeightLearner  # Learns heuristic weights from episodes via least-squares regression
â”œâ”€â”€ AdaptiveSAAgent        # Simulated annealing agent with adaptive exploration
â”œâ”€â”€ SmartCurriculumManager # Automatically adapts difficulty based on past performance
â”œâ”€â”€ train_with_adaptive_curriculum() # Main training loop
â”œâ”€â”€ evaluate_comprehensive()         # Testing and performance benchmarking
â””â”€â”€ plot_training_progress()        # (Optional) training plots
```

---

## ğŸ“Š Example Output

```text
Episode 150:
  Difficulty: 30 shuffle steps
  Success Rate: 0.80
  Avg Steps (successful): 212.5
  Weights: min=0.95, max=2.85, mean=1.34
  Recent loss: 0.0421
```

Plotting shows learning curves over episodes:

* ğŸ“ˆ Success rate over time
* ğŸ“‰ Average steps (if successful)
* ğŸ§® Loss convergence
* ğŸ§Š Learned heuristic heatmaps

---

## ğŸ§ª Evaluation

Compares performance of:

* Standard heuristic (Manhattan + linear conflicts)
* Learned heuristic (adaptive weights based on position)

Metrics:

* âœ… Solve rate (%)
* â± Avg steps to solution
* ğŸ” Timeouts

---

## ğŸ¤ Contributing

1. Fork the project
2. Create a feature branch (`git checkout -b feature/my-feature`)
3. Commit changes (`git commit -am 'Add new feature'`)
4. Push to branch (`git push origin feature/my-feature`)
5. Create a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## âœ¨ Acknowledgments

Inspired by curriculum learning in reinforcement learning and heuristic optimization techniques in combinatorial problems.

---


